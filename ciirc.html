<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CIIRC Experience - Filip Ručka</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style> 
        body { font-family: 'Inter', sans-serif; } 
        .prose p { margin-bottom: 1rem; }
        .prose h4 { font-size: 1.25rem; font-weight: 700; margin-top: 2rem; margin-bottom: 1rem; color: #1e293b; }
        .prose ul { list-style-position: inside; margin-left: 1rem; margin-bottom: 1rem; }
        .prose li { margin-bottom: 0.5rem; }
        .prose code { 
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
            background-color: #f1f5f9;
            color: #e11d48;
            padding: 0.2em 0.4em;
            border-radius: 0.25rem;
            font-size: 0.9em;
        }
        .active-nav-text {
            color: #2563eb; /* text-blue-600 */
            transform: translateX(5px);
            font-weight: 600;
        }
    </style>
</head>
<body class="bg-slate-50 text-slate-700">
    <div class="container mx-auto max-w-screen-lg px-6 py-12 md:py-20 lg:py-24">
        
        <a href="./index.html" class="group flex items-center font-semibold text-blue-600 mb-12">
            <i class="fas fa-arrow-left mr-2 transition-transform group-hover:-translate-x-1"></i>
            Go Back to Portfolio
        </a>

        <header class="mb-12">
            <h1 class="text-4xl font-bold text-slate-900 sm:text-5xl">Computer Vision Software Engineer at CIIRC CTU</h1>
            <p class="mt-3 text-lg text-slate-600">Developing computer vision systems for robotic perception and manipulation within the DeKrys project.</p>
        </header>
        <div class="lg:flex lg:justify-between lg:gap-16">
            <!-- LEFT STICKY SIDEBAR -->
            <aside class="lg:sticky lg:top-24 lg:w-1/3 h-full">
                <h3 class="text-lg font-bold text-slate-900 mb-4 border-b border-slate-200 pb-2">Key Details</h3>
                <dl class="text-sm">
                    <dt class="font-semibold text-slate-800">Role:</dt>
                    <dd class="text-slate-600 mb-3">Undergraduate Researcher (Robotic Perception)</dd>
                    
                    <dt class="font-semibold text-slate-800">Duration:</dt>
                    <dd class="text-slate-600 mb-3">[Start Date] — Present</dd>
                    
                    <dt class="font-semibold text-slate-800">Technologies:</dt>
                    <dd class="text-slate-600">
                        <ul class="list-disc list-inside">
                            <li>Python</li>
                            <li>OpenCV</li>
                            <li>NumPy</li>
                            <li>Shapely</li>
                            <li>alphashape</li>
                        </ul>
                    </dd>
                </dl>
                
                <!-- On this page navigation -->
                <nav class="hidden md:block mt-12">
                    <h3 class="text-lg font-bold text-slate-900 mb-4 border-b border-slate-200 pb-2">On this page</h3>
                    <ul id="nav-menu" class="flex flex-col space-y-3 text-sm font-medium mt-4">
                        <li><a href="#project-overview" class="group flex items-center transition-all duration-300 text-slate-600 hover:text-slate-900"><span class="nav-indicator mr-4 h-px w-8 bg-slate-400 transition-all duration-300 group-hover:w-16 group-hover:bg-slate-800"></span><span class="nav-text">Project Overview</span></a></li>
                        <li><a href="#macro-processing" class="group flex items-center transition-all duration-300 text-slate-600 hover:text-slate-900"><span class="nav-indicator mr-4 h-px w-8 bg-slate-400 transition-all duration-300 group-hover:w-16 group-hover:bg-slate-800"></span><span class="nav-text">Macro Image Processing</span></a></li>
                        <li><a href="#change-detection" class="group flex items-center transition-all duration-300 text-slate-600 hover:text-slate-900"><span class="nav-indicator mr-4 h-px w-8 bg-slate-400 transition-all duration-300 group-hover:w-16 group-hover:bg-slate-800"></span><span class="nav-text">Change Detection</span></a></li>
                        <li><a href="#alignment" class="group flex items-center transition-all duration-300 text-slate-600 hover:text-slate-900"><span class="nav-indicator mr-4 h-px w-8 bg-slate-400 transition-all duration-300 group-hover:w-16 group-hover:bg-slate-800"></span><span class="nav-text">Template Matching</span></a></li>
                    </ul>
                </nav>
            </aside>
            
          <!-- RIGHT SCROLLABLE CONTENT -->
          <main class="lg:w-2/3 mt-8 lg:mt-0">
            <div class="prose prose-slate max-w-none">
                <h2 id="project-overview" class="text-3xl font-bold text-slate-900 scroll-mt-24">Project Overview</h2>
                <p>
                    This project, developed for the DeKrys initiative, involved creating a robust Python-based system to precisely estimate the position and orientation of crystals for robotic manipulation. The vision pipeline processes multiple camera views a high resolution macro image of the crystal and "before" and "after" images from a destination camera to achieve accurate localization and placement validation.
                </p>
                <p>
                    The system is designed as a modular class-based interface, providing key information such as the crystal's final position, its rotation, and potential collisions with other crystals on the destination plate.
                </p>

                <!-- FLOW DIAGRAM IMAGE -->
                <figure class="my-8">
                    <img src="flowdiagram+.pdf" alt="Flow diagram of the crystal detection pipeline" class="rounded-lg w-full shadow-lg border border-slate-200">
                    <figcaption class="text-sm text-center text-slate-500 mt-3">
                        A high-level overview of the vision pipeline's logic, from image input to final validation.
                    </figcaption>
                </figure>
                
                <h2 id="macro-processing" class="text-3xl font-bold text-slate-900 mt-8 mb-4 scroll-mt-24">1. Macro Image Processing</h2>
                <p>
                    The process begins by analyzing the high-resolution macro image to extract the crystal's unique shape.
                </p>
                <h4>Edge Detection & Alpha Shape</h4>
                <p>
                    Instead of simple thresholding, the system uses Sobel filters to compute a gradient magnitude image, effectively highlighting the crystal's edges. A concave hull (<strong>alpha shape</strong>) is then generated from these edge points. This creates a tight, non-convex polygon that accurately represents the crystal's irregular contour, providing a far more precise model than a simple bounding box.
                </p>
                <h4>Pipette Identification</h4>
                <p>
                    To locate the robotic pipette holding the crystal, a circle-fitting algorithm is applied to the alpha shape's boundary. A sliding window approach tests segments of the contour, fitting circles using a least-squares method. The best-fit circle that meets specific radius and overlap criteria is identified as the pipette, allowing its position to be tracked relative to the crystal.
                </p>

                <h2 id="change-detection" class="text-3xl font-bold text-slate-900 mt-8 mb-4 scroll-mt-24">2. Change Detection in Placer Images</h2>
                <p>
                    To find exactly where the crystal was placed, the system analyzes the destination camera images by isolating only the areas that have changed.
                </p>
                <h4>Difference Masking</h4>
                <p>
                    By computing a pixel-wise difference between the "before" and "after" grayscale images, we isolate the exact region of change. This difference image is then blurred to reduce noise and thresholded to create a clean binary mask. This critical step focuses the subsequent search area only on where the new crystal has appeared, improving efficiency and reducing false positives.
                </p>
                
                <h2 id="alignment" class="text-3xl font-bold text-slate-900 mt-8 mb-4 scroll-mt-24">3. Template Matching & Alignment</h2>
                <p>
                    Next is to align the crystal's shape model (from the macro image) with its detected location in the destination image to determine its final orientation and position.
                </p>
                <h4>Iterative Rotational Matching</h4>
                <p>
                    The crystal's alpha shape model is used as a template. A multi-step rotational search is performed where the template is rotated at progressively finer increments (e.g., from coarse 5° steps down to fine 0.1° steps). At each angle, the algorithm scans the template across the placer's difference mask to find the highest correlation score. This iterative approach determines the crystal's precise final orientation and position.
                </p>
                <figure class="my-8">
                    <img src="coords.png" alt="Coordinate system for image processing. Image by Ing. Pavel Krsek Ph.D." class="rounded-lg w-full shadow-lg border border-slate-200">
                    <figcaption class="text-sm text-center text-slate-500 mt-3">
                        Coordinate system for image processing. Image by Ing. Pavel Krsek Ph.D.
                    </figcaption>
                </figure>
                <h4>Final Validation and Output</h4>
                <p>
                    Once the best alignment is found, the system transforms the crystal's contour and the pipette's position into the destination camera's coordinate system. This data is used to generate a final overlay for visual verification and, critically, to perform <strong>collision checks</strong> against previously placed crystals using geometric libraries.
                </p>

                At the end, the code calculates the orientation and positional difference between the projection points and the detected points by the algorigthm. It uses Principal Component Analysis to find the main axis of each point set and then calculates the angle between these axes in degrees. This tells us how much one set of points is rotated relative to the other. By doing this, we obtain the shift between the centers of the two point sets and their rotation.
                <figure class="my-8">
                    <img src="pca.png" alt="Determining the position of the detected crystal relative to the planned position. Image by Ing. Pavel Krsek Ph.D." class="rounded-lg w-full shadow-lg border border-slate-200">
                    <figcaption class="text-sm text-center text-slate-500 mt-3">
                        Determining the position of the detected crystal relative to the planned position. Image by Ing. Pavel Krsek Ph.D.
                    </figcaption>
                </figure>
             </div>
        </main>
        </div>
    </div>

<script>
    document.addEventListener('DOMContentLoaded', () => {
        const sections = document.querySelectorAll('main h2[id]');
        const navLinks = document.querySelectorAll('#nav-menu a');

        if (sections.length === 0 || navLinks.length === 0) {
            return;
        }

        const observer = new IntersectionObserver((entries) => {
            let currentActiveId = '';
            
            let maxVisibleRatio = -1;
            sections.forEach(section => {
                const rect = section.getBoundingClientRect();
                const topBoundary = 0;
                const bottomBoundary = window.innerHeight / 2;
                const visibleHeight = Math.max(0, Math.min(rect.bottom, bottomBoundary) - Math.max(rect.top, topBoundary));
                const visibleRatio = visibleHeight / rect.height;

                if (visibleRatio > maxVisibleRatio) {
                    maxVisibleRatio = visibleRatio;
                    currentActiveId = section.id;
                }
            });

            navLinks.forEach(link => {
                const navText = link.querySelector('.nav-text');
                const navIndicator = link.querySelector('.nav-indicator');
                const linkId = link.getAttribute('href').substring(1);

                if (linkId === currentActiveId) {
                    navText.classList.add('active-nav-text');
                    navIndicator.classList.add('w-16', 'bg-slate-800');
                    navIndicator.classList.remove('w-8', 'bg-slate-400');
                } else {
                    navText.classList.remove('active-nav-text');
                    navIndicator.classList.remove('w-16', 'bg-slate-800');
                    navIndicator.classList.add('w-8', 'bg-slate-400');
                }
            });
        }, { 
            root: null,
            threshold: Array.from(Array(101).keys(), i => i / 100)
        });

        sections.forEach(section => observer.observe(section));
    });
</script>

</body>
</html>
